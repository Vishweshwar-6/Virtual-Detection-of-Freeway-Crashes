{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b088e7e3-f9bc-457b-92a2-3b0f7d9e9a3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d090d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import math\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import time, date, timedelta\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e6bc66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining directories \n",
    "## Place change the directory as requred\n",
    "ipath= \"EE 526 Project Dataset and Results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfe833e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Querring Wejo connected vehicle data from \"Intrans\" athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8075e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selected crash locations, \n",
    "crs=pd.read_csv(path+\"Crash Locations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definig SQL query for exact measure and generating required fields\n",
    "\n",
    "query = \"\"\"\n",
    "select datapointid, journeyid, capturedtimestamp, latitude, longitude,  speed, acceleration, heading, route_id,  dist+segment_start_measure as measure,  year, month, day, hour \n",
    "from (\n",
    "    SELECT * , round((ST_Distance(ST_POINT(t.longitude,t.latitude), ST_Point(t.segment_start_longitude,t.segment_start_latitude))*69.047),5) dist  \n",
    "    from (\n",
    "        select datapointid, journeyid, capturedtimestamp, latitude, longitude, geohash, postalcode, speed, acceleration, heading, vm.route_id, distance_from_route, vm.segment_start_measure, vm.segment_end_measure, segment_start_longitude, segment_start_latitude, year, quarter, vm.county, month, day, hour \n",
    "        from (\n",
    "            select * from \"wejo_vm\".\"2022_q4\" where county = %(cnt)s and month = %(mo)s and day= %(dy)s and (hour between %(hr)s-2 and %(hr)s +2) and route_id = %(rid)s and (segment_start_measure between %(sm)s - 2 and %(sm)s + 2) and (distance_from_route between -50 and 50 )) vm\n",
    "            left join \"lrs_geohash\".\"ft_cnt_gh_bearing\" lrs \n",
    "                    on lrs.geohash_lrs = vm.geohash and lrs.route_id = vm.route_id and lrs.county = vm.county and \n",
    "                    (lrs.segment_start_measure between vm.segment_start_measure-0.002 and vm.segment_start_measure+0.002) \n",
    "                    and (lrs.segment_end_measure between vm.segment_end_measure-0.002 and vm.segment_end_measure+0.002)\n",
    "            ) t\n",
    "        )\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988b7ef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Extracting wejo connected vehicle data in a loop for every crash at the defined boundaries\n",
    "st = time.time()\n",
    "cry=crs[(crs[\"year\"]==2022)]\n",
    "for m in range(10,13):\n",
    "    crm=cry[(cry[\"month\"]==m)].reset_index(inplace=False)\n",
    "    for i in range(0,len(crm)):\n",
    "        cnt = crm.at[i,\"NAME\"].upper()\n",
    "        dy = crm.at[i,\"day\"]\n",
    "        rid = crm.at[i,\"ROUTE_ID\"]\n",
    "        ck = crm.at[i,\"atmid\"]\n",
    "        hr = crm.at[i,\"hour\"]\n",
    "        sm = crm.at[i,\"seg_st_mea\"]\n",
    "        dicn = {'mo' : m,'cnt': str(cnt), 'dy': int(dy), 'rid':str(rid), 'sm':float(sm), 'hr': int(hr)}\n",
    "        wejo_df=as_pandas(cursor.execute(query,dicn))\n",
    "        \n",
    "        wejo_df.to_csv(path+'Data/Crash Locations/'+str(ck)+'-wejo.csv',index=False) # The storage location is not provided as these csv files are not submitted  \n",
    "        \n",
    "        print('Execution time for month',m, \"Key\", ck, \":\", time.time() - st, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0aad3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4 hour images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1108148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(ipath + \"Evaluation/Data/*.csv\") # this location is also not provided as these files are not submitted, final images are submitted \n",
    "li = []\n",
    "for filename in files:\n",
    "    fn=filename[-19:-9]\n",
    "    try:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        df['capturedtimestamp'] = pd.to_datetime(df['capturedtimestamp'])\n",
    "        df1=df[[ 'journeyid', 'capturedtimestamp', 'measure']]\n",
    "        df1.set_index('capturedtimestamp', inplace=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(150, 50))\n",
    "        df1.groupby('journeyid')['measure'].plot(linewidth=4)\n",
    "        ax.xaxis.set_major_locator(mdates.MinuteLocator(byminute=[0,15,30,45], interval = 1))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "        plt.rc('xtick',labelsize=50)\n",
    "        plt.rc('ytick',labelsize=50)\n",
    "        plt.grid()\n",
    "        plt.savefig(ipath+\"Evaluation/Images/\"+str(fn)+\"-Trajectory.jpg\",bbox_inches='tight', dpi=100)\n",
    "\n",
    "        print(\"completed: \",fn) #, end=\"\\r\"\n",
    "    except:\n",
    "        li.append(fn)\n",
    "        print(\"Failed: \",fn) # , end=\"\\r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45950582-14e6-4974-85db-b0df9e13846b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84cbab99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Geneerating overall crash scenario Plots for all locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08fa657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing crash data\n",
    "crs=pd.read_csv(path+\"Crash Locations.csv\")\n",
    "crs['datetime'] = pd.to_datetime(crs['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efc4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calling each file saved from the previous section and plotting the trajectories and saving them in a driectory\n",
    "failed = []\n",
    "st = time.time()\n",
    "cry=crs[(crs[\"year\"]==2022)]\n",
    "for m in range(10,11):\n",
    "    crm=cry[(cry[\"month\"]==m)].reset_index(inplace=False)\n",
    "    for i in range(0,len(crm)):\n",
    "        ck = crm.at[i,\"atmid\"]\n",
    "        ct = crm.at[i,\"datetime\"]\n",
    "        try:\n",
    "            df = pd.read_csv(path+'Data/Crash Locations/'+str(ck)+'-wejo.csv')\n",
    "            df['capturedtimestamp'] = pd.to_datetime(df['capturedtimestamp'])\n",
    "            df1=df[[ 'journeyid', 'capturedtimestamp', 'measure']]\n",
    "            df1.set_index('capturedtimestamp', inplace=True)\n",
    "        \n",
    "            fig, ax = plt.subplots(figsize=(150, 50))\n",
    "            df1.groupby('journeyid')['measure'].plot(linewidth=4)\n",
    "            ax.xaxis.set_major_locator(mdates.MinuteLocator(byminute=[0,15,30,45], interval = 1))\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "            plt.axvline(ct,color='b', linewidth=10,linestyle='--',ymax =50, label='ATMS_Crash_Time: {}'.format(ct))\n",
    "            plt.rc('xtick',labelsize=50)\n",
    "            plt.rc('ytick',labelsize=50)\n",
    "            plt.grid()\n",
    "            plt.savefig(ipath+\"Train&Test Dataset Images/\"+str(ck)+\"-Trajectory.jpg\",bbox_inches='tight', dpi=100)\n",
    "            ## These images are not submitted as these are used for annottation. Final image dataset is submitted directly\n",
    "#             plt.show()\n",
    "            print('Execution time for month',m, \"Key\", ck, \":\", time.time() - st, 'seconds', ct)\n",
    "        except:\n",
    "            failed.append(ck)\n",
    "            print('failed',m, \"Key\", ck, \":\", time.time() - st, 'seconds', ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90732ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. Generating training images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21d1e4-ef15-4176-8658-a708b93cfde0",
   "metadata": {},
   "source": [
    "After Careful manual annotations of more than 1000 images from the plots generated above, The training image dataset is generated using the following sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0324dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing annotations and coberting time stamps\n",
    "annotations = pd.read_csv(ipath+\"Train&Test Dataset Annotations.csv\")\n",
    "annotations[\"ST\"] = annotations.apply(lambda x: time(x.Start_Hour,x.Start_Minute,0,0),axis=1)\n",
    "annotations[\"ET\"] = annotations.apply(lambda x: time(x.End_Hour, x.End_Minute,0,0),axis=1)\n",
    "annotations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bde81d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Running for loop on annotation file to generate training images and saving in corresponding directories\n",
    "failed = []\n",
    "for i in range(0, len(annotations)):\n",
    "    ck = annotations.at[i,\"Crash_Key\"]\n",
    "    cr = annotations.at[i,\"Crash\"].upper()\n",
    "    st = annotations.at[i,\"ST\"]\n",
    "    et = annotations.at[i,\"ET\"]\n",
    "    sm = annotations.at[i,\"Start_Measure\"]\n",
    "    em = annotations.at[i,\"End_Measure\"]\n",
    "    try:\n",
    "        df = pd.read_csv(ipath+\"Data/\"+str(ck)+\"-wejo.csv\")\n",
    "        df['capturedtimestamp'] = pd.to_datetime(df['capturedtimestamp'])\n",
    "        df[\"time\"] = df['capturedtimestamp'].apply(lambda x: x.time() )\n",
    "        df1 = df[((df[\"time\"]>= st) & (df[\"time\"]<= et)) & ((df[\"measure\"]>= sm) & (df[\"measure\"]<= em))] \n",
    "        df1=df1[[ 'journeyid', 'capturedtimestamp', 'measure']]\n",
    "        df1 =df1.sort_values(by = [\"journeyid\",'capturedtimestamp'])\n",
    "        \n",
    "        jids= df1[[ 'journeyid', 'measure']].groupby('journeyid').std().reset_index()\n",
    "        jds= np.unique(jids[jids[\"measure\"]>0.02][\"journeyid\"])\n",
    "        df2 = df1[df1[\"journeyid\"].isin(jds)==True]\n",
    "        \n",
    "        jids1= df2[[ 'journeyid', 'measure']].groupby('journeyid').count().reset_index()\n",
    "        jds1 = np.unique(jids1[jids1[\"measure\"] > 4][\"journeyid\"])\n",
    "        df3 = df2[df2[\"journeyid\"].isin(jds1)==True]\n",
    "        \n",
    "        df3.set_index('capturedtimestamp', inplace=True)\n",
    "        fig, ax = plt.subplots(figsize=(50, 30))\n",
    "        df3.groupby('journeyid')['measure'].plot(linewidth=4,color = 'black')\n",
    "        plt.savefig(ipath+\"Train&Test Dataset Images/\"+str(cr)+\"/\"+str(ck)+\"-\"+str(i)+\".jpg\",bbox_inches='tight', dpi=100)\n",
    "#         plt.show()\n",
    "        print(\"Completed: \", i, ck , end = '\\r')\n",
    "    except:\n",
    "        failed.append(i)\n",
    "        print(\"failed: \",i, ck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0597d6-ca04-4c0b-a67b-6e1ec9b9551e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Convolutiona Nueral Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eaf59e-b969-4b06-815c-aabc91a3e83b",
   "metadata": {},
   "source": [
    "We used Convolution Neural Networks(CNN) for image classification. In order to go further for model building we need to have a tensorflow environment. The following are the steps need to be followed to enable tensorflow environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec1837-f7a3-430b-9569-284a5451c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to have numpy version which is less that 1.20.0 \n",
    "#to run the below mentioned tensor flow model image data generator.\n",
    "!pip install numpy==1.19.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd098e-c58c-4a4f-a52a-994f1692b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c35a2-194b-4b85-9cf9-7bb488dff20e",
   "metadata": {},
   "source": [
    "This section of code(below) is used for constructing a Convolutional Neural Network (CNN) using Keras. \n",
    "Each line of the code adds a different layer to the CNN, designed for image recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1d3bb-8f80-4dff-abf0-9eab84a30641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Sequential model\n",
    "# This will serve as the foundation for our neural network,\n",
    "# where layers will be added in a linear fashion.\n",
    "classifier=Sequential()\n",
    "\n",
    "#Layer 1\n",
    "classifier.add(Convolution2D(32,(3,3),input_shape=(224,224,3), activation='relu')) #line adds the first convolution layer with 32 filters of size 3x3. The input_shape=(224,224,3) specifies that our input will be 224x224 RGB images (3 channels).\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))# max pooling layer with a pool size of 2x2. \n",
    "\n",
    "#layer 2\n",
    "classifier.add(Convolution2D(32,(3,3), activation='relu'))#line adds the first convolution layer with 32 filters of size 3x3. The input_shape=(224,224,3) specifies that our input will be 224x224 RGB images (3 channels).\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))# max pooling layer with a pool size of 2x2.\n",
    "\n",
    "#Layer 3\n",
    "classifier.add(Convolution2D(64,(3,3), activation='relu'))# increases the number of filters to 64\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))# max pooling layer with a pool size of 2x2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523034c4-1256-41f9-9f9b-464bfed84688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the 2D feature maps into a 1D vector for the dense layers\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7326c1-00a3-41e5-b0d8-0f60ee8d4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a dense layer with 32 units and ReLU activation for further processing\n",
    "classifier.add(Dense(units=32,activation='relu'))\n",
    "\n",
    "# Adding another dense layer with 64 units and ReLU activation for complexity\n",
    "classifier.add(Dense(units=64,activation='relu'))\n",
    "\n",
    "# Increasing to 128 units in the dense layer for more complex feature learning\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "\n",
    "# Expanding to 256 units in the dense layer to enhance model's learning capacity\n",
    "classifier.add(Dense(units=256,activation='relu'))\n",
    "\n",
    "# Final dense layer with 2 units and sigmoid activation for binary classification\n",
    "classifier.add(Dense(units=2,activation='sigmoid'))\n",
    "\n",
    "# Compiling the model with the Adam optimizer, categorical cross-entropy loss, and tracking accuracy\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faec36e-dde6-4562-8770-ae768b9d89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing ImageDataGenerator for data augmentation and preprocessing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Setting up the image data generator for training data with augmentation parameters\n",
    "train_datagen=ImageDataGenerator(rescale=1./255,\n",
    "                                 shear_range=0.15,\n",
    "                                 zoom_range=0.2,\n",
    "                                 horizontal_flip=True)\n",
    "\n",
    "# Setting up the image data generator for test data with only rescaling\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87917d0f-2a6e-4bb0-936b-4e5e560e3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data paths\n",
    "train_path_dir=r\"C:\\Users\\*****\\Train&Test Dataset Images\\Train\"\n",
    "test_path_dir=r\"C:\\Users\\*****\\Train&Test Dataset Images\\Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab823b93-893f-4992-95dc-2c2ed3a1ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datasets for model training.\n",
    "training_set=train_datagen.flow_from_directory(train_path_dir,\n",
    "                                               target_size=(224,224),\n",
    "                                               batch_size=20,\n",
    "                                               class_mode='categorical')\n",
    "\n",
    "test_set=test_datagen.flow_from_directory(test_path_dir,target_size=(224,224),batch_size=10,class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db55ec-fc1d-4a41-a342-18ee202c6d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model training set for 20 epochs and each each epoch is have 30 steps.\n",
    "x=classifier.fit_generator(training_set,\n",
    "                        steps_per_epoch=30,\n",
    "                        epochs=20,\n",
    "                        validation_data=test_set,\n",
    "                        validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714e75c-7b26-4a93-8d78-508816401f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "classifier.save('Crash_non_crash.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaebc18f-721a-4f42-b725-b4ece6099526",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6 Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843bd04-25f2-4bea-a489-add49ca99dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model\n",
    "model = load_model('Crash_non_crash.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08494ea3-83dd-4547-90f6-7c7edee119b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and predict an image\n",
    "def predict_image(img_path):\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array /= 255.0  # Rescale pixel values\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(img_array)\n",
    "\n",
    "    # Translate prediction to class label\n",
    "    if prediction[0][0] > prediction[0][1]:\n",
    "        return \"crash\"\n",
    "    else:\n",
    "        return \"non-crash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494188a7-311e-4334-ad4b-c11ade4915a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mention the test dataset path for identification of either Crash or Non-Crash\n",
    "path=r\"C:\\Users\\*****\\Train&Test Dataset Images\\Test\\Crash\"\n",
    "images=os.listdir(path)\n",
    "for pic in images:\n",
    "    image_path_address=os.path.join(path,pic)\n",
    "    image_path = image_path_address\n",
    "    result = predict_image(image_path)\n",
    "    print(f\"The image {pic} is classified as {result} image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdb8c76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce69d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7.1 Generating Five-min trahectory images from 20 crashes other than training crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb5c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Calling the evaluation annotations for 20 crashes selected\n",
    "annotations = pd.read_csv(ipath+\"Evaluation Images.csv\")\n",
    "annotations[\"ST\"] = annotations.apply(lambda x: time(int(x.Start_Hour),int(x.Start_Minute),0),axis=1)\n",
    "annotations[\"ET\"] = annotations.apply(lambda x: time(int(x.End_Hour), int(x.End_Minute),0,0),axis=1)\n",
    "annotations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7cf9f7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Running for loop on evaluation annotation file to generate evaluation images at one minute interval for five_minute time periods and saving in corresponding directories\n",
    "\n",
    "failed = []\n",
    "for i in range(0, len(annotations)):\n",
    "    ck = annotations.at[i,\"Crash_Key\"]\n",
    "    st = annotations.at[i,\"ST\"]\n",
    "    et = annotations.at[i,\"ET\"]\n",
    "    t2 = (dt.datetime.combine(dt.date(1,1,1), st) + timedelta(minutes=10)).time()\n",
    "    sm = annotations.at[i,\"Start_Measure\"]\n",
    "    em = annotations.at[i,\"End_Measure\"]\n",
    "    d = annotations.at[i,\"Crash_time\"]\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(ipath+\"Evaluation Images\"+str(ck)+\"-wejo.csv\")\n",
    "        df['capturedtimestamp'] = pd.to_datetime(df['capturedtimestamp'])\n",
    "        df[\"time\"] = df['capturedtimestamp'].apply(lambda x: x.time())\n",
    "        df1 = df[((df[\"time\"]>= st) & (df[\"time\"]<= et)) & ((df[\"measure\"]>= sm) & (df[\"measure\"]<= em))] \n",
    "        df1=df1[[ 'journeyid', 'time', 'measure']]\n",
    "        df1 =df1.sort_values(by = [\"journeyid\",'time'])\n",
    "\n",
    "        jids= df1[[ 'journeyid', 'measure']].groupby('journeyid').std().reset_index()\n",
    "        jds= np.unique(jids[jids[\"measure\"]>0.01][\"journeyid\"])\n",
    "        df2 = df1[df1[\"journeyid\"].isin(jds)==True]\n",
    "\n",
    "        jids1= df2[[ 'journeyid', 'measure']].groupby('journeyid').count().reset_index()\n",
    "        jds1 = np.unique(jids1[jids1[\"measure\"] > 1][\"journeyid\"])\n",
    "        df3 = df2[df2[\"journeyid\"].isin(jds1)==True]\n",
    "\n",
    "        c=1\n",
    "\n",
    "        while t2<=et:\n",
    "            dfat=df3[(df3[\"time\"]>=st) & (df3[\"time\"]<=t2)]\n",
    "            dfat.set_index('time', inplace=True)\n",
    "            fig, ax = plt.subplots(figsize=(50, 30))\n",
    "            dfat.groupby('journeyid')['measure'].plot(linewidth=4, color = 'black')\n",
    "            plt.savefig(ipath+\"Evaluation Images/\"+str(ck)+\"-\"+str(c)+\"-\"+str(d)+\"-\"+str(t2.hour)+\"-\"+str(t2.minute)+\".jpg\",bbox_inches='tight', dpi=100)\n",
    "\n",
    "            st = (dt.datetime.combine(dt.date(1,1,1), st) + timedelta(minutes=1)).time()\n",
    "            t2 = (dt.datetime.combine(dt.date(1,1,1), t2) + timedelta(minutes=1)).time()\n",
    "            c+=1\n",
    "            print(\"Completed: \", ck ,i, t2, et, end = '\\r')\n",
    "    except:\n",
    "        failed.append(i)\n",
    "        print(\"failed: \",i, ck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c066f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining function for crash prediction\n",
    "\n",
    "def predict_image(img_path):\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0) \n",
    "    img_array /= 255.0 \n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(img_array)\n",
    "\n",
    "    # Translate prediction to class label\n",
    "    if prediction[0][0] > prediction[0][1]:\n",
    "        return \"crash\"\n",
    "    else:\n",
    "        return \"non-crash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c481fd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "evaluation =pd.DataFrame(columns= [\"Crash_Key\",\"Sequence\",\"Measure\",\"Hour\",\"Minute\", \"Prediction\"])\n",
    "\n",
    "files = glob.glob(ipath + \"Evaluation Images/*\")\n",
    "for filename in files:\n",
    "    fn=filename.split(\"\\\\\")[-1].split('.')[0]\n",
    "    pred = predict_image(filename)\n",
    "#     print(pred)\n",
    "    evaluation.loc[len(evaluation),:] = [fn.split(\"-\")[0], fn.split(\"-\")[1], fn.split(\"-\")[2], fn.split(\"-\")[3], fn.split(\"-\")[4], pred]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
